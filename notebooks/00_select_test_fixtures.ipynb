{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omop_alchemy import get_engine_name, load_environment, TEST_PATH, ROOT_PATH\n",
    "from orm_loader.helpers import get_logger\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "# old enumerator classes from monolithic version of omop_alchemy - selection of cancer-relevant codes\n",
    "import concept_enums\n",
    "\n",
    "base_path = TEST_PATH / \"fixtures\" / \"athena_source\"\n",
    "load_dotenv()\n",
    "source_path = Path(os.getenv('SOURCE_PATH', 'update/path/to/athena/source/as/required'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b63035",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = pd.read_csv(source_path / 'CONCEPT.csv', delimiter='\\t', low_memory=False)\n",
    "concept_class = pd.read_csv(source_path / 'CONCEPT_CLASS.csv', delimiter='\\t')\n",
    "relationship = pd.read_csv(source_path / 'RELATIONSHIP.csv', delimiter='\\t')\n",
    "domain = pd.read_csv(source_path / 'DOMAIN.csv', delimiter='\\t')\n",
    "vocabulary = pd.read_csv(source_path / 'VOCABULARY.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff8c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_concepts = set(concept_class.concept_class_concept_id) | set(relationship.relationship_concept_id) | set(domain.domain_concept_id) | set(vocabulary.vocabulary_concept_id)\n",
    "required_concepts_df = concept[concept.concept_id.isin(required_concepts)]\n",
    "\n",
    "selected = []\n",
    "for d in set(domain.domain_id):\n",
    "    try:\n",
    "        c = concept[(concept.domain_id == d) & (concept.standard_concept == 'S')]\n",
    "        selected.append(c.sample(min(50, len(c)), random_state=1))\n",
    "    except ValueError:\n",
    "        print(f\"Not enough standard concepts in domain {d}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b273fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_concept_by_domain_df = pd.concat(selected)\n",
    "\n",
    "additional_test_concepts = set([x for y in \n",
    "                                    [concept_enums.__dict__[cls].member_values() \n",
    "                                        for cls in dir(concept_enums) \n",
    "                                        if hasattr(concept_enums.__dict__[cls], 'member_values')\n",
    "                                    ] \n",
    "                                for x in y])\n",
    "\n",
    "additional_test_concept_df = concept[concept.concept_id.isin(additional_test_concepts)]\n",
    "\n",
    "metadata = concept[concept.domain_id == 'Metadata']\n",
    "language = concept[concept.domain_id == 'Language']\n",
    "locations = concept[(concept.concept_class_id=='Location') & (concept.standard_concept.notna())].sample(frac=0.1, replace=False)\n",
    "\n",
    "additional_cancer_ones = []\n",
    "\n",
    "for vocab, frac in {'Cancer Modifier': 1.0, 'HemOnc': 0.1, 'ICDO3': 0.05}.items():\n",
    "    additional_cancer_ones.append(concept[(concept.vocabulary_id == vocab) & concept.standard_concept.notna()].sample(frac=frac, replace=False))\n",
    "\n",
    "cancer_specific_df = pd.concat(additional_cancer_ones)\n",
    "\n",
    "selected_concept_df = pd.concat(\n",
    "    [\n",
    "        standard_concept_by_domain_df,\n",
    "        required_concepts_df,\n",
    "        additional_test_concept_df,\n",
    "        cancer_specific_df,\n",
    "        locations,\n",
    "        metadata,\n",
    "        language\n",
    "    ]\n",
    ").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_relationships = []\n",
    "\n",
    "for concept_rel in pd.read_csv(source_path / 'CONCEPT_RELATIONSHIP.csv', delimiter='\\t', low_memory=False, chunksize=100000):\n",
    "    filtered = concept_rel[\n",
    "        (concept_rel.concept_id_1.isin(selected_concept_df.concept_id)) &\n",
    "        (concept_rel.concept_id_2.isin(selected_concept_df.concept_id))\n",
    "    ]\n",
    "    if not filtered.empty:\n",
    "        selected_relationships.append(filtered)\n",
    "\n",
    "selected_ancestry = []\n",
    "\n",
    "for concept_anc in pd.read_csv(source_path / 'CONCEPT_ANCESTOR.csv', delimiter='\\t', low_memory=False, chunksize=100000):\n",
    "    filtered = concept_anc[\n",
    "        (concept_anc.ancestor_concept_id.isin(selected_concept_df.concept_id)) &\n",
    "        (concept_anc.descendant_concept_id.isin(selected_concept_df.concept_id))\n",
    "    ]\n",
    "    if not filtered.empty:\n",
    "        selected_ancestry.append(filtered)\n",
    "\n",
    "selected_synonyms = []\n",
    "\n",
    "for concept_syn in pd.read_csv(source_path / 'CONCEPT_SYNONYM.csv', delimiter='\\t', low_memory=False, chunksize=100000):\n",
    "    filtered = concept_syn[\n",
    "        (concept_syn.concept_id.isin(selected_concept_df.concept_id))\n",
    "    ]\n",
    "    if not filtered.empty:\n",
    "        selected_synonyms.append(filtered)\n",
    "\n",
    "\n",
    "selected_relationship_df = pd.concat(selected_relationships)\n",
    "selected_ancestry_df = pd.concat(selected_ancestry)\n",
    "selected_synonyms_df = pd.concat(selected_synonyms)\n",
    "\n",
    "\n",
    "selected_relationship_df.to_csv(base_path / 'CONCEPT_RELATIONSHIP.csv', sep='\\t', index=False)\n",
    "selected_synonyms_df.to_csv(base_path / 'CONCEPT_SYNONYM.csv', sep='\\t', index=False)\n",
    "selected_ancestry_df.to_csv(base_path / 'CONCEPT_ANCESTOR.csv', sep='\\t', index=False)\n",
    "selected_concept_df.to_csv(base_path / 'CONCEPT.csv', sep='\\t', index=False)\n",
    "domain.to_csv(base_path / 'DOMAIN.csv', sep='\\t', index=False)\n",
    "vocabulary.to_csv(base_path / 'VOCABULARY.csv', sep='\\t', index=False)\n",
    "relationship.to_csv(base_path / 'RELATIONSHIP.csv', sep='\\t', index=False)\n",
    "concept_class.to_csv(base_path / 'CONCEPT_CLASS.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c1353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f5be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [domain, vocabulary, relationship, concept_class, selected_relationship_df, selected_ancestry_df, selected_synonyms_df]:\n",
    "    for col in f.columns:\n",
    "        if 'concept_id' in col:\n",
    "            if len(f[~f[col].isin(selected_concept_df.concept_id)]) > 0:\n",
    "                raise ValueError(f\"Found concept_id in {col} not in selected concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(selected_relationship_df[~selected_relationship_df.relationship_id.isin(relationship.relationship_id.unique())]) == 0, \"Found relationship_id not in selected relationships\"\n",
    "assert len(concept[~concept.concept_class_id.isin(concept_class.concept_class_id.unique())]) == 0, \"Found concept_class_id not in selected concepts\"\n",
    "assert len(concept[~concept.domain_id.isin(domain.domain_id.unique())]) == 0, \"Found domain_id not in selected domains\"\n",
    "assert len(concept[~concept.vocabulary_id.isin(vocabulary.vocabulary_id.unique())]) == 0, \"Found vocabulary_id not in selected vocabularies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65cc24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [selected_concept_df, domain, vocabulary, relationship, concept_class, selected_relationship_df, selected_ancestry_df]:\n",
    "    assert(len(f[f.duplicated()]) == 0), f\"Found duplicated rows in {f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97014890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the import issue...TODO: add pk null normalisation on load\n",
    "vocabulary.loc[vocabulary.vocabulary_id.isna(), 'vocabulary_id'] = 'Unknown_Vocabulary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322e679f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff54924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[metadata.concept_id==1147138]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc803944",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_concept_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb592e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.get('SOURCE_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7cfd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omop-alchemy (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
